---
title: "Assignment5"
author: "ZHU MAO"
date: "October 26, 2016"
output: html_document
---
1 Method 1
1.1 Project data 
```{r}
datapath <- "C:/Users/AA/Desktop/Statistical Analysis/Class5/"
dat <- read.csv(file = paste(datapath, "ResidualAnalysisProjectData_2.csv", sep = "/"), header = TRUE, sep = ",")
head(dat)

plot(dat$Input, dat$Output, type = "p", pch = 19)

nSample <- length(dat$Input)
```


1.2 Estimate linear model 
Fit linear model to the data and plot the sample and the fitted values. 
```{r}
m1 <- lm(Output ~ Input, dat) 
m1$coefficients

matplot(dat$Input, cbind(dat$Output, m1$fitted.values), type = "p", pch = 16,
        ylab = "Sample and Fitted Values")
```

Analyze results of fitting. 
```{r}
summary(m1)
```
Interpret the summary of the model: 

For the Residuals, the distribution is quite even and normal with median close to 0. The coefficient for the Intercept is not significant with p-value larger than 0.05, but the coefficient for the slope for Input variable is siginificant with small p-value, and from the F test we could also see small p-value indicating the significant of the estimated slope. The R squared is 0.835, which is a good level indicating that the model is a good fit, and 83.50% variations in the Output could be explained by the Input variable.   

Analyze the residuals, plot them. 
```{r}
estimatedResiduals <- m1$residuals
plot(dat$Input, estimatedResiduals)
```

And their probability density function. 
```{r}
Probability.Density.Residuals <- density(estimatedResiduals)
plot(Probability.Density.Residuals, ylim = c(0,0.5))
lines(Probability.Density.Residuals$x, dnorm(Probability.Density.Residuals$x,
                                             mean = mean(estimatedResiduals),
                                             sd = sd(estimatedResiduals)))
```
What does the pattern of residuals and the pattern of the data tell you about the sample?
From the data: It seems that when Input is larger than 5, the distribution of data is more spread out compared with the fitting line. 
From the residual: close to normal distribution, but it has some variations around 0. 

What kind of mixture of two models do you see in the data?
It is possible that the models are combined with two models generated by linear regression but with slightly different slopes. 


1.3 Creating training sample for separation of mixed models
Creating training sample with Input >= 5 and separate the points above the fitted line and below.
```{r}
Train.Sample <- data.frame(trainInput = dat$Input, trainOutput = rep(NA, nSample))
Train.Sample.Steeper <- data.frame(trainSteepInput = dat$Input, 
                                      trainSteepOutput = rep(NA, nSample))
Train.Sample.Flatter <- data.frame(trainFlatInput = dat$Input, 
                                      trainFlatOutput = rep(NA, nSample))

head(cbind(dat, Train.Sample, Train.Sample.Steeper, Train.Sample.Flatter))
```

Select parts of the sample with Input greater than 5 and Output either above the estimated regression line or below it.
```{r}
# Create Selectors 
Train.Sample.Selector <- dat$Input >= 5
Train.Sample.Steeper.Selector <- Train.Sample.Selector & (dat$Output > m1$fitted.values)
Train.Sample.Flatter.Selector <- Train.Sample.Selector & (dat$Output <= m1$fitted.values)
```

Create training samples for steep and flat slopes.
```{r}
# Select subsamples 
Train.Sample[Train.Sample.Selector,2] <- dat[Train.Sample.Selector,2]
Train.Sample.Steeper[Train.Sample.Steeper.Selector, 2] <- dat[Train.Sample.Steeper.Selector,2]
Train.Sample.Flatter[Train.Sample.Flatter.Selector,2] <- dat[Train.Sample.Flatter.Selector,2]
head(Train.Sample)
```
Check what are the resulting training samples. 
```{r}
head(cbind(dat,
           Train.Sample,
           Train.Sample.Steeper,
           Train.Sample.Flatter),10)
```
```{r}
plot(Train.Sample$trainInput,Train.Sample$trainOutput, pch = 16, ylab = "Training
     Sample Output", xlab = "Training Sample Input")
points(Train.Sample.Steeper$trainSteepInput, Train.Sample.Steeper$trainSteepOutput,
       pch=20, col = "green")
points(Train.Sample.Flatter$trainFlatInput, Train.Sample.Flatter$trainFlatOutput,
       pch = 20, col = "blue")
```


1.4 Fit linear models to train samples
Fit linear models to both training samples, interpret the summaries of both models. 
```{r}
Train.Sample.Steep.lm <- lm(Train.Sample.Steeper$trainSteepOutput~Train.Sample.Steeper$trainSteepInput
                            ,data = Train.Sample.Steeper)

summary(Train.Sample.Steep.lm)$coefficients

summary(Train.Sample.Steep.lm)$sigma

summary(Train.Sample.Steep.lm)$df

summary(Train.Sample.Steep.lm)$r.squared

summary(Train.Sample.Steep.lm)$adj.r.squared

summary(Train.Sample.Steep.lm)$fstatistic
```
```{r}
Train.Sample.Flat.lm <- lm(Train.Sample.Flatter$trainFlatOutput~Train.Sample.Flatter$trainFlatInput
                            ,data = Train.Sample.Flatter)

summary(Train.Sample.Flat.lm)$coefficients

summary(Train.Sample.Flat.lm)$sigma

summary(Train.Sample.Flat.lm)$df

summary(Train.Sample.Flat.lm)$r.squared

summary(Train.Sample.Flat.lm)$adj.r.squared

summary(Train.Sample.Flat.lm)$fstatistic
```
Print out the coefficients of both models for the training sample. 
```{r}
rbind(Steeper.Coefficients = Train.Sample.Steep.lm$coefficients,
      Flatter.Coefficients = Train.Sample.Flat.lm$coefficients)
```
Plot the entire sample with the fitted regression lines estimated from both training samples. 
```{r}
plot(dat$Input, dat$Output, type = "p", pch = 19)
lines(dat$Input,predict(Train.Sample.Steep.lm, data.frame(trainSteepInput = dat$Input),
                        interval = "prediction")[,1], col = "red",lwd = 3)
   
lines(dat$Input,predict(Train.Sample.Flat.lm, data.frame(trainFlatInput = dat$Input),
                        interval = "prediction")[,1], col = "green",lwd = 3)

```
Separate the entire sample using the estimated train linear models. 
Define distances from each point to both regression lines. 
```{r}
# Define distances from each Output point to both estimated training lines
Distances.to.Steeper <- abs(dat$Output - dat$Input*Train.Sample.Steep.lm$coefficients[2]-
                                Train.Sample.Steep.lm$coefficients[1])
Distances.to.Flatter <- abs(dat$Output - dat$Input*Train.Sample.Flat.lm$coefficients[2]-
                                Train.Sample.Flat.lm$coefficients[1])
```
Define separating sequence which equals TRUE if observation belongs to model with steeper slope and FALSE otherwise. 
```{r}
# Define the unscramble sequence 
Unscrambling.Sequence.Steeper <- Distances.to.Steeper < Distances.to.Flatter
```
Separate the data into steeper and flatter parts. Create data frames. 
```{r}
# Define two subsampleswith NAs in the Output columns. 
Subsample.Steeper <- data.frame(steeperInput = dat$Input, steeperOutput = rep(
    NA,nSample))
Subsample.Flatter <- data.frame(flatterInput = dat$Input, flatterOutput = rep(
    NA, nSample))
```
Fill in data frames.
```{r}
# Fill in the unscrambled outputs instead of NAs where necessary. 
Subsample.Steeper[Unscrambling.Sequence.Steeper,2]<-dat[Unscrambling.Sequence.Steeper,2]
Subsample.Flatter[!Unscrambling.Sequence.Steeper,2] <-dat[!Unscrambling.Sequence.Steeper,2]

# Check the first rows
head(cbind(dat, Subsample.Steeper, Subsample.Flatter))
```
Plot the two samples. 
```{r}
# Plot the unscrambled subsamples, include the original entire sample as a check.
matplot(dat$Input, cbind(dat$Output,
                         Subsample.Steeper$steeperOutput,
                         Subsample.Flatter$flatterOutput),
        type = "p", col = c("black", "green", "blue"),
        pch = 16, ylab = "Separated Subsamples")
```
Find mixing probability. 
```{r}
# Mixing Probability of Steeper Slope. 
(Mixing.Probability.Of.Steeper.Slope <- sum(Unscrambling.Sequence.Steeper)/
    length(Unscrambling.Sequence.Steeper))
```
Run binomial test for the null hypothesis p=0.5 and two-sided alternative "p is not equal to 0.5". Interpret the output of binom.test
```{r}
binom.test(x = sum(Unscrambling.Sequence.Steeper), n = nSample, p = 0.5,
           alternative = c("two.sided"),
           conf.level = 0.95)
```
Since p-value is less than 0.05, so that we could reject the null hypothesis which is p=0.5, which means the probability for data in the steeper slope subsample is differnt from 0.5. 


1.5 Fitting models to separated samples
Estimate linear models for separated subsamples. 
```{r}
Linear.Model.Steeper.Recovered <- lm(Subsample.Steeper$steeperOutput~Subsample.Steeper$steeperInput,
                                     data = Subsample.Steeper)
Linear.Model.Flatter.Recovered <- lm(Subsample.Flatter$flatterOutput~Subsample.Flatter$flatterInput,
                                     data = Subsample.Flatter)
```
Print out coefficients for both separated models. Check the summaries.
```{r}
rbind(Steeper.Coefficients = Linear.Model.Steeper.Recovered$coefficients,
      Flatter.Coefficients = Linear.Model.Flatter.Recovered$coefficients)

summary(Linear.Model.Steeper.Recovered)$r.sq

summary(Linear.Model.Flatter.Recovered)$r.sq
```


1.6 Analyze the residuals
Compare the residuals of separated models with the residuals of the single model. 
```{r}
matplot(dat$Input, cbind(c(summary(Linear.Model.Steeper.Recovered)$residuals,
                           summary(Linear.Model.Flatter.Recovered)$residuals),
                         estimatedResiduals), type = "p", pch = c(19,16), 
        ylab = "Residuals before and after unscrambling")
legend("bottomleft", legend = c("Before", "After"), col = c("red", "black"),
          pch = 16)
```
Estimate standard deviations of the residuals. 
```{r}
# Estimate standard deviations
unmixedResiduals <- c(summary(Linear.Model.Steeper.Recovered)$residuals,
                      summary(Linear.Model.Flatter.Recovered)$residuals)
apply(cbind(ResidualsAfter = unmixedResiduals, 
            ResidualsBefore = estimatedResiduals),2,sd)
```

Check assumptions about residuals. 
```{r}
suppressWarnings(library(fitdistrplus))
```
```{r}
hist(unmixedResiduals)
```
```{r}
(residualsParam <- fitdistr(unmixedResiduals,"normal"))
```
```{r}
ks.test(unmixedResiduals,"pnorm", residualsParam$estimate[1], residualsParam$estimate[2])
```
```{r}
ks.test(unmixedResiduals,"pnorm", residualsParam$estimate[1], residualsParam$estimate[2])
```
Finally, print out the slopes and intercepts of both models. 
```{r}
# Slopes
c(Steeper.Slope = Linear.Model.Steeper.Recovered$coefficients[2], 
  Flatter.Slope = Linear.Model.Flatter.Recovered$coefficients[2])
```
```{r}
#Intercepts 
c(Steeper.Intercept = Linear.Model.Steeper.Recovered$coefficients[1],
  Flatter.Intercept = Linear.Model.Flatter.Recovered$coefficients[1])
```



2. Alternative Method Based on Volatility Clustering 
```{r}
plot(dat$Input, (dat$Output - mean(dat$Output))^2, type = "p", pch = 19,
                 ylab = "Squared Deviations") 
```
Explain how increased slope affects variance of the output and the pattern of variables zi. 
The variance of the output increased as well with increased slope and the distribution of zi becomes more spread out. 

What are the differences between the shapes of parabolas corresponding to a steeper versus flatter slope? 
With a flatter(smaller) slope, the shape of parabola is more clustered together at the bottom and flat and widely spread. With a steeper(larger) slope, the shape of parabola is more lepukurtic, clustered to the center and steeper. 

Separate the models using this approach. 
Find parabola corresponding to fitted model m1. 
Hint. Find mean(y) using model expression. 
```{r}
# m1 <- lm(Output ~ Input, dat)
# m1$coefficients

yMean <- m1$coefficients[2] * mean(dat$Input) + m1$coefficients[1]

yi <- m1$coefficients[2] * (dat$Input) + m1$coefficients[1]

clusteringParabola <- (yi - yMean)^2

plot(dat$Input, (dat$Output - mean(dat$Output))^2, type = "p", pch = 19, 
     ylab = "Squared Deviations")

points(dat$Input, clusteringParabola,pch = 19, col = "red")
```
Define the separating sequence Unscrambling.Sequence.Steeper.var, such that it is equal to TRUE for steeper slope subsample and FLASE for flatter slope subsample. 
```{r}
Unscrambling.Sequence.Steeper.var <- rep(FALSE, 1000)

for (i in 1:length(Unscrambling.Sequence.Steeper)){
    if ((dat$Output[i] - mean(dat$Output))^2 > clusteringParabola[i]) 
        {Unscrambling.Sequence.Steeper.var[i] = TRUE}}

head(Unscrambling.Sequence.Steeper.var,10)
```
Separate the sample into steeper and flatter part. Create data frames. Define two subsamples with NAs in the Output columns. 
```{r}
Subsample.Steeper.var <- data.frame(steeperInput.var = dat$Input, 
                                    steeperOutput.var = rep(NA, nSample))
Subsample.Flatter.var <- data.frame(flatterInput.var = dat$Input,
                                    flatterOutput.var = rep(NA, nSample))
```
Fill in the unscrambled outputs instead of NAs where necessary. 
```{r}
Subsample.Steeper.var[Unscrambling.Sequence.Steeper.var,2] <- dat[Unscrambling.Sequence.Steeper.var,2]
Subsample.Flatter.var[!Unscrambling.Sequence.Steeper.var,2] <- dat[!Unscrambling.Sequence.Steeper.var,2]

#Check the first 10 rows 
head(cbind(dat, Subsample.Steeper.var, Subsample.Flatter.var), 10)
```
Plot clusters of the variance data and the separating parabola
```{r}
plot(dat$Input,(dat$Output - mean(dat$Output))^2, type = "p", pch = 19, 
     ylab = "Squared Deviations")
points(dat$Input, clusteringParabola, pch = 19, col = "red")
points(dat$Input[Unscrambling.Sequence.Steeper.var],
       (dat$Output[Unscrambling.Sequence.Steeper.var] - 
           mean(dat$Output))^2, pch = 19, col = "blue")
points(dat$Input[!Unscrambling.Sequence.Steeper.var], 
       (dat$Output[!Unscrambling.Sequence.Steeper.var] - 
            mean(dat$Output))^2, pch = 19, col = "green")
```
Plot the unscrambled subsamples, include the original entire sample as a check. 
```{r}
excludeMiddle <- (dat$Input <= mean(dat$Input) - 0)|
                  (dat$Input >= mean(dat$Input) + 0)
matplot(dat$Input[excludeMiddle],cbind(dat$Output[excludeMiddle],
                                       Subsample.Steeper.var$steeperOutput.var[excludeMiddle],
                                       Subsample.Flatter.var$flatterOutput.var[excludeMiddle]),
         type = 'p', col = c("black","green","blue"),
         pch = 16, ylab = "Separated Subsamples")
```
Note that observations corresponding to the minimum of the variance dat are difficult to separate. Consider omitting some observations around that point. 
For example, make omitted interval equal to LeftBound = -0.5, RightBound = 0.5. 
```{r}
excludeMiddle1 <- (dat$Input <= mean(dat$Input) - 0.5)|
    (dat$Input >= mean(dat$Input) + 0.5)
matplot(dat$Input[excludeMiddle1],cbind(dat$Output[excludeMiddle1],
                                       Subsample.Steeper.var$steeperOutput.var[excludeMiddle1],
                                       Subsample.Flatter.var$flatterOutput.var[excludeMiddle1]),
        type = 'p', col = c("black","green","blue"),
        pch = 16, ylab = "Separated Subsamples")
```
Fit linear models to the separated samples. 
```{r}
SteeperInput <- dat$Input[excludeMiddle1]
SteeperOutput <- Subsample.Steeper.var$steeperOutput.var[excludeMiddle1]
dat.Steep.var <- lm(SteeperOutput~SteeperInput)

FlatterInput <- dat$Input[excludeMiddle1]
FlatterOutput <- Subsample.Flatter.var$flatterOutput.var[excludeMiddle1]
dat.Flat.var <- lm(FlatterOutput~FlatterInput)
```

Plot the data and the estimated regression line. 
```{r}
plot(dat$Input, dat$Output, type = 'p', pch = 19)

lines(SteeperInput,
      dat.Steep.var$coefficients[2]*SteeperInput+dat.Steep.var$coefficients[1],
      col = "red", lwd = 3)
lines(FlatterInput,
      dat.Flat.var$coefficients[2]*FlatterInput+dat.Flat.var$coefficients[1],
      col = "green", lwd = 3)
```

Print estimated parameters and summaries of both models. 
```{r}
rbind(Steeper.Coefficients.var = dat.Steep.var$coefficients,
      Flatter.Coefficients.var = dat.Flat.var$coefficients)
```
```{r}
summary(dat.Steep.var)
```
```{r}
summary(dat.Flat.var)
```

Plot residuals from the combined model and the models for separated samples. 
```{r}
matplot(dat$Input[excludeMiddle],
        cbind(c(summary(dat.Steep.var)$residuals,
                summary(dat.Flat.var)$residuals),
              estimatedResiduals[excludeMiddle]),
        type = "p", pch = c(19,16), ylab = "Residuals before and after unscrabling")

```

3. Answer the Question on Slide 10 of the Lecture Notes. 
The two should be the same. 
Slope_Estimate = cov(y,x)/var(x) != sum(yi(xi-xbar))/sum((xi-xbar)^2). 
Because
cov(y,x) = (sum(yi-ybar)(xi-xbar))/(n-1) 
var(x) = sum(xi-xbar)^2/(n-1)

???(yi-ybar)(xi-xbar) = sum(yi(xi-xbar))-sum(ybar(xi-xbar))
                    = sum(yi(xi-xbar))-ybar*sum(xi-xbar)
                    = sum(yi(xi-xbar))-ybar*(sum(xi) - sum(xbar))
                    = sum(yi(xi-xbar))-ybar(nxbar - nxbar)
                    = sum(yi(xi-xbar)) 

cov(y,x) = (sum(yi-ybar)(xi-xbar))/(n-1) 
         = sum(yi(xi-xbar)/(n-1))
         
Slope_Estimate = cov(y,x)/var(x) 
      = (sum(yi(xi-xbar)/(n-1))/(sum(xi-xbar)^2/(n-1))
      = sum(yi(xi-xbar))/sum((xi-xbar)^2)
      
So the two should be the same instead of different. 

I then use dataset "dat"" to prove in R. Same results could be get. 
```{r}
Slope_Estimate1 <- sum(dat$Output*(dat$Input - mean(dat$Input)))/
                  sum((dat$Input - mean(dat$Input))^2)

Slope_Estimate2 <- cov(dat$Output,dat$Input)/var(dat$Input)

rbind(Slope_Estimate1,Slope_Estimate2)
```


4. Test
Create datapath and read the data: 
```{r}
dataPath <- "C:/Users/AA/Desktop/Statistical Analysis/Class5/"

dat1 <- read.table(paste(dataPath,'Week5_Test_Sample.csv',sep = '/'),header =TRUE)
```

Plot the dataset to take a look at the distribution
```{r}
plot(dat1$Input, dat1$Output, type = "p", pch = 19)

nSampleTest <- length(dat1$Input)
```

Generate the regression 
```{r}
GeneralModel <- lm(dat1$Output ~ dat1$Input, data = dat1)
```

Take a look at the summary of the model and coefficients
```{r}
summary(GeneralModel)
```
From the distribution of the residuals we could see that the median is pretty close to 0, which means that the distribution is ok. For the coefficients of Intercept and slope, both are statistically significant. R-square value is 0.9184, which proves a good fit, and from F-statistic we could also tell the significance of the slope.  

Plot the residuals
```{r}
Estimated.Residuals1 <- GeneralModel$residuals

plot(dat1$Input,Estimated.Residuals1)

Probability.Density.Residuals1 <- density(Estimated.Residuals1)

plot(Probability.Density.Residuals1, ylim = c(0, 1))

lines(Probability.Density.Residuals1$x, 
      dnorm(Probability.Density.Residuals1$x, 
      mean = mean(Estimated.Residuals1), 
      sd = sd(Estimated.Residuals1)))

```
The distribution of residual is not quite normal with some bimodal turbulance around the mean, the data could be devided into two subsamples for further analysis. 

Plot the squared deviations
```{r}
yMean1 <- GeneralModel$coefficients[2] * mean(dat1$Input) + GeneralModel$coefficients[1]

yFitted <- GeneralModel$coefficients[2] * (dat1$Input) + GeneralModel$coefficients[1]

clusteringParabola1 <- (yFitted - yMean1)^2

plot(dat1$Input, (dat1$Output - mean(dat1$Output))^2, type = "p", pch = 19, 
     ylab = "Squared Deviations")

points(dat1$Input, clusteringParabola1,pch = 19, col = "red")
```

Define the separating sequence, and make it TRUE for steeper subsample and 
FALSE for flatter subsample.
```{r}
Unscrambling.Sequence.Steeper.var1 <- rep(FALSE, 1000)

for (i in 1:nSampleTest){
    if ((dat1$Output[i] - mean(dat1$Output))^2 > clusteringParabola1[i]) 
    {Unscrambling.Sequence.Steeper.var1[i] = TRUE}}

```

Separate the sample into steeper and flatter part. 
```{r}
Subsample.Steeper.var1 <- data.frame(steeperInput.var1 = dat1$Input, 
                                    steeperOutput.var1 = rep(NA, nSample))
Subsample.Flatter.var1<- data.frame(flatterInput.var1 = dat1$Input,
                                    flatterOutput.var1 = rep(NA, nSample))

```

Fill in the unscrambled outputs instead of NAs where necessary.
```{r}
Subsample.Steeper.var1[Unscrambling.Sequence.Steeper.var1,2] <- 
    dat1[Unscrambling.Sequence.Steeper.var1,1]

Subsample.Flatter.var1[!Unscrambling.Sequence.Steeper.var1,2] <- 
    dat1[!Unscrambling.Sequence.Steeper.var1,1]
```

Plot clusters of the variance data and the separating parabola
```{r}
plot(dat1$Input,(dat1$Output - mean(dat1$Output))^2, type = "p", pch = 19, 
     ylab = "Squared Deviations")
points(dat1$Input, clusteringParabola1, pch = 19, col = "red")
points(dat1$Input[Unscrambling.Sequence.Steeper.var1],
       (dat1$Output[Unscrambling.Sequence.Steeper.var1] - 
            mean(dat1$Output))^2, pch = 19, col = "blue")
points(dat1$Input[!Unscrambling.Sequence.Steeper.var1], 
       (dat1$Output[!Unscrambling.Sequence.Steeper.var1] - 
            mean(dat1$Output))^2, pch = 19, col = "green")
```

Plot the unscrambled subsamples, include the original entire sample as a check. 
```{r}
matplot(dat1$Input, cbind(dat1$Output,
                         Subsample.Steeper.var1$steeperOutput.var1,
                         Subsample.Flatter.var1$flatterOutput.var1),
        type = "p", col = c("black", "green", "blue"),
        pch = 16, ylab = "Separated Subsamples")
```
```{r}
excludeMiddle2 <- (dat1$Input <= mean(dat1$Input) - 0)|
    (dat1$Input >= mean(dat1$Input) + 0)

matplot(dat1$Input[excludeMiddle2],cbind(dat1$Output[excludeMiddle2],                                   Subsample.Steeper.var1$steeperOutput.var1[excludeMiddle2],
                                       Subsample.Flatter.var1$flatterOutput.var1[excludeMiddle2]),
        type = 'p', col = c("black","green","blue"),
        pch = 16, ylab = "Separated Subsamples")
```

Fit linear models to the separated samples. 
```{r}
SteeperInput1 <- dat1$Input[excludeMiddle2]
SteeperOutput1 <- Subsample.Steeper.var1$steeperOutput.var1[excludeMiddle2]
mSteep <- lm(SteeperOutput1~SteeperInput1)

FlatterInput1 <- dat1$Input[excludeMiddle2]
FlatterOutput1 <- Subsample.Flatter.var1$flatterOutput.var1[excludeMiddle2]
mFlat <- lm(FlatterOutput1~FlatterInput1)

```

Print summaries of both models.
```{r}
summary(mSteep)

summary(mFlat)
```
From both summaries we could be able to see that the R-squared increased for both subsamples compared with the original single sample. 

Plot residuals from the combined model and the models for separated samples. 
```{r}
matplot(dat1$Input[excludeMiddle2],
        cbind(c(summary(mSteep)$residuals,
                summary(mFlat)$residuals),
              estimatedResiduals[excludeMiddle2]),
        type = "p", pch = c(19,16), ylab = "Residuals before and after unscrabling")

```
The residuals decreased after we divide the single sample group into two subsamples. 

```{r}
res <- list(GeneralModel = GeneralModel, mSteep = mSteep, mFlat = mFlat)

saveRDS(res,file = paste(dataPath, 'result.rds', sep = '/'))
```



















































































